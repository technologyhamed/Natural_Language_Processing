{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzPXHCfX2sOvdV+w+z84qf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/technologyhamed/Natural_Language_Processing/blob/main/Named_Entity_Recognition_(NER)_model__BERT_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "U13BqAASk9my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HgkfQKSSX9SK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample annotated data (replace with your actual data)\n",
        "data = [\n",
        "    {\"tokens\": [\"John\", \"Doe\", \"is\", \"from\", \"New\", \"York\"], \"labels\": [0, 0, 1, 1, 0, 0]},\n",
        "    {\"tokens\": [\"Jane\", \"Smith\", \"lives\", \"in\", \"London\"], \"labels\": [0, 0, 1, 1, 0]}\n",
        "]\n",
        "\n",
        "# Convert to DataFrame and then to Dataset\n",
        "df = pd.DataFrame(data)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)  # Adjust num_labels\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "tDWZj3zjj3Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xu8rAcWlw58",
        "outputId": "d1892735-a27f-48f7-8346-29a439bfffb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train and validation sets\n",
        "train_dataset, validation_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Use the split training set\n",
        "    eval_dataset=validation_dataset,  # Use the split validation set\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "ryArGV8RmJMg",
        "outputId": "cbb106c9-36d4-4996-b4a4-265cd2f7855f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.960298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.842150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.795064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=0.8712170918782552, metrics={'train_runtime': 28.2736, 'train_samples_per_second': 0.106, 'train_steps_per_second': 0.106, 'total_flos': 12248396208.0, 'train_loss': 0.8712170918782552, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval  # Install the missing library"
      ],
      "metadata": {
        "id": "N7MG_N8Jm1q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metric\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "# Compute metrics\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "H_lZ1PGGmlBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}